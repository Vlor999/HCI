\section{Evaluation}

\subsection{Evaluation Methodology}

Our evaluation employed a multi-pronged strategy to assess both technical performance and user experience. We used four main methodologies: automated testing, explanation quality assessment, user studies, and performance metrics analysis.
Automated unit tests, integrated into our CI pipeline, ensured reliability across components such as input processing, reasoning algorithms, and output generation. For explanation quality, we combined keyword matching and semantic similarity metrics to evaluate clarity, relevance, and accuracy.
User studies engaged participants in realistic tasks, providing feedback on usability and comprehension. Performance evaluation focused on response time, answer accuracy, and user satisfaction, prioritizing semantic correctness over speed.

\subsection{Quantitative Results}
We used keyword coverage (targeting $\geq$75\%) as a primary metric for explanation completeness. Factual accuracy was validated through expert review and knowledge base cross-checks, yielding high correctness rates across query types.
We also evaluated consistency by testing the system with semantically similar queries. Results showed stable performance, though some variability suggested potential for improvement through additional fine-tuning.
\subsection{Qualitative Findings}

Users valued clear, jargon-free explanations in natural language and appreciated responses tailored to context. Context-aware answers enhanced understanding and reduced cognitive load.
Complex scenarios sometimes led to less focused responses, while simple cases produced consistently clear outputs. Interactive capabilities, such as follow-up questions, increased user trust and engagement.

\subsection{Limitations and Challenges}

Key limitations included high computational costs for real-time explanations and occasional over-reliance on rigid templates. Context retention weakened in long conversations, affecting coherence.
Finally, domain-specific fine-tuning remains essential for high performance in specialized fields, as general models lacked sufficient adaptability to expert terminology and expectations.
